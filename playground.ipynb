{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import sklearn.naive_bayes as sn\n",
    "\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g. \n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    '''\n",
    "    tokens = []\n",
    "    # YOUR CODE HERE\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        # remove stop_words (commonly used meaningless words) and numbers\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_bagofwords(data, vocab_dict):\n",
    "    '''\n",
    "    :param data: a list of words, type: list\n",
    "    :param vocab_dict: a dict from words to indices, type: dict\n",
    "    return a word (sparse) matrix, type: scipy.sparse.csr_matrix\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html\n",
    "    '''\n",
    "    # use of linked list to create spare matrix\n",
    "    data_matrix = sparse.lil_matrix((len(data), len(vocab_dict)))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc:\n",
    "            # dict.get(key, -1)\n",
    "            # if the word in the vocab_dic, return the value\n",
    "            # else return -1\n",
    "            word_idx = vocab_dict.get(word, -1)\n",
    "            if word_idx != -1:\n",
    "                data_matrix[i, word_idx] += 1\n",
    "\n",
    "    # csr: row based format, better for matrix multiplication\n",
    "    # to speed up when computing\n",
    "    data_matrix = data_matrix.tocsr()\n",
    "\n",
    "    return data_matrix\n",
    "\n",
    "def read_data(file_name, vocab=None):\n",
    "    \"\"\"\n",
    "    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].apply(tokenize)\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = set()\n",
    "        for i in range(len(df)):\n",
    "            for word in df.iloc[i]['words']:\n",
    "                vocab.add(word)\n",
    "\n",
    "    # dictionary of vocab : index_num\n",
    "    vocab_dict = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "    data_matrix = get_bagofwords(df['words'], vocab_dict)\n",
    "\n",
    "    return df['id'], df['label'], data_matrix, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['words'] = df['text'].map(lambda x: x.lower()) \n",
    "    df['words'] = df['words'].str.replace('[^\\w\\s]', '')  \n",
    "    df['words'] = df['words'].apply(nltk.word_tokenize)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    df['words'] = df['words'].apply(lambda x: [stemmer.stem(y) for y in x])  \n",
    "    # This converts the list of words into space-separated strings\n",
    "    df['words'] = df['words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    count_vect = CountVectorizer()  \n",
    "    counts = count_vect.fit_transform(df['words'])  \n",
    "    transformer = TfidfTransformer().fit(counts)\n",
    "    \n",
    "    counts = transformer.transform(counts)\n",
    "    return df, counts, transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 10)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "df = pd.read_csv('Project 1 Sentiment Classification/data/train.csv')\n",
    "df['words'] = df['text'].map(lambda x: x.lower()) \n",
    "df['words'] = df['words'].str.replace('[^\\w\\s]', '')  \n",
    "df['words'] = df['words'].apply(tokenize)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# df['words'] = df['words'].apply(lambda x: [stemmer.stem(y) for y in x])  \n",
    "# This converts the list of words into space-separated strings\n",
    "df['words'] = df['words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a student, by back and neck are under constant strain and Dr Serrick manages to set them straight every time. I highly recommend the thumper treatment for really great muscle pain relief. Overall I would say that Dr Serrick is very knowledgeable, empathetic, and thorough. Highly recommended.  :  5.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(df['text'][0], \" : \", df['stars'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_csv(fp, np): \n",
    "    d_f = pd.read_csv(fp)\n",
    "    stars = d_f['stars'].apply(int)\n",
    "    result = pd.concat([stars, d_f['text']], axis=1)\n",
    "    \n",
    "    with open(np, 'w') as f: \n",
    "        result.to_csv(f, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_csv('Project 1 Sentiment Classification/data/train.csv', 'mod_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_csv('Project 1 Sentiment Classification/data/valid.csv', 'mod_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mod_valid.csv', 'w') as f: \n",
    "    result.to_csv(f, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mod_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(comment):\n",
    "    # preprocessing using regular expression\n",
    "    \n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;.,]\", \" \", \n",
    "        str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    return [\n",
    "        x.text for x in NLP.tokenizer(comment) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:03<00:00, 410.84it/s]\n"
     ]
    }
   ],
   "source": [
    "v_count = {} \n",
    "\n",
    "for text in tqdm(df['text']): \n",
    "    for t in tokenizer(text): \n",
    "        if t not in v_count: \n",
    "            v_count[t] = 1\n",
    "        else:\n",
    "            v_count[t] +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_v = list(v_count.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_v.sort(key=lambda r: r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thumper', 1),\n",
       " ('50gal', 1),\n",
       " ('5gpm', 1),\n",
       " ('tommaso', 1),\n",
       " ('saucing', 1),\n",
       " ('integrative', 1),\n",
       " ('personalised', 1),\n",
       " ('3514', 1),\n",
       " ('hygeine', 1),\n",
       " ('evading', 1),\n",
       " ('modes', 1),\n",
       " ('expaned', 1),\n",
       " ('thouroughly', 1),\n",
       " ('nitrite', 1),\n",
       " ('infuriatingly', 1),\n",
       " ('johnstown', 1),\n",
       " ('jalap', 1),\n",
       " ('demerara', 1),\n",
       " (\"everyone'd\", 1),\n",
       " ('hums', 1),\n",
       " ('funnily', 1),\n",
       " (\"d'backs\", 1),\n",
       " ('nack', 1),\n",
       " ('bullcrap', 1),\n",
       " ('gastroparesis', 1),\n",
       " ('titilate', 1),\n",
       " ('heloc', 1),\n",
       " ('150k', 1),\n",
       " ('okkkkkkkk', 1),\n",
       " ('hospitaliser', 1),\n",
       " ('commodités', 1),\n",
       " ('banque', 1),\n",
       " ('nationale', 1),\n",
       " ('bucanero', 1),\n",
       " ('nista', 1),\n",
       " ('ingreds', 1),\n",
       " ('flavorable', 1),\n",
       " ('aaactually', 1),\n",
       " ('stretchinnng', 1),\n",
       " ('hrmmmmm', 1),\n",
       " ('muuuuuuuuha', 1),\n",
       " ('haaah', 1),\n",
       " ('spookily', 1),\n",
       " ('narrate', 1),\n",
       " ('larious', 1),\n",
       " ('bordello', 1),\n",
       " ('opp', 1),\n",
       " ('pshaw', 1),\n",
       " ('yip', 1),\n",
       " ('laxatives', 1),\n",
       " ('マッカラン空港に近い、ラスベガスのストリップ大通り南側にあります', 1),\n",
       " ('エクスカリバー、ピラミッド形のラクソーとグループのホテルが３つ並んでいて、３つのホテルの間は無料のモノレールが利用できます。エクスカリバーの交差点を北側へ行けば、ニューヨークニューヨークを経てベラッジオやシザーズパレスへ徒歩で行けます。ストリップ大通りには市バスが頻繁に走っているので、疲れたらバスに乗るとよいです',\n",
       "  1),\n",
       " ('マンダレイベイは、単にカジノホテルというだけでなく、巨大な水族館やリゾート感満載なプール、全体的にトロピカルな雰囲気で統一されているホテルです',\n",
       "  1),\n",
       " ('ここは大きな展示会場があり、２００８年と２０１０年の２回、アメリカ食品小売業教会（fmi）の展示会が開催されました。それまでシカゴ・マコーミックプレイスで開催されていましたが、この２回と２０１２年のダラスだけはシカゴ以外で開催されました。直近の２０１４年のfmiショーはシカゴ開催に戻っています',\n",
       "  1),\n",
       " ('fergus', 1),\n",
       " ('snarly', 1),\n",
       " ('booom', 1),\n",
       " ('spottings', 1),\n",
       " ('maylani', 1),\n",
       " ('pinapply', 1),\n",
       " ('jammin', 1),\n",
       " ('kirsti', 1),\n",
       " ('cheweyness', 1),\n",
       " ('heinously', 1),\n",
       " ('tches', 1),\n",
       " (\"l'entrepôt\", 1),\n",
       " ('connaissez', 1),\n",
       " ('bruit', 1),\n",
       " (\"l'abris\", 1),\n",
       " ('brouhaha', 1),\n",
       " (\"l'écart\", 1),\n",
       " ('égale', 1),\n",
       " ('dispendieuse', 1),\n",
       " ('aient', 1),\n",
       " ('formulations', 1),\n",
       " ('ruccola', 1),\n",
       " ('oilve', 1),\n",
       " ('affidavits', 1),\n",
       " ('materially', 1),\n",
       " ('fetunta', 1),\n",
       " ('892', 1),\n",
       " ('huice', 1),\n",
       " ('sumptious', 1),\n",
       " ('scribe', 1),\n",
       " ('picada', 1),\n",
       " ('pandebono', 1),\n",
       " ('michene', 1),\n",
       " ('givng', 1),\n",
       " ('calculators', 1),\n",
       " ('s&t', 1),\n",
       " ('momiji', 1),\n",
       " ('magnificant', 1),\n",
       " ('8599', 1),\n",
       " ('1663604', 1),\n",
       " ('etoufe', 1),\n",
       " ('~cheap', 1),\n",
       " ('~typical', 1),\n",
       " ('~not', 1),\n",
       " ('~great', 1),\n",
       " (\"~don't\", 1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_v[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_v = [t for t in sorted_v if t[0] not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81961"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36133\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for word in sorted_v: \n",
    "    if word[1] == 1:\n",
    "        count+= 1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('Project 1 Sentiment Classification/data/test.csv')\n",
    "test['words'] = test['text'].map(lambda x: x.lower()) \n",
    "test['words'] = test['words'].str.replace('[^\\w\\s]', '')  \n",
    "test['words'] = test['words'].apply(tokenize)\n",
    "\n",
    "# test['words'] = test['words'].apply(lambda x: [stemmer.stem(y) for y in x])  \n",
    "# This converts the list of words into space-separated strings\n",
    "test['words'] = test['words'].apply(lambda x: ' '.join(x))    \n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [ (1, 1), (1, 2), (2, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__C': [1, 10, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial')),\n",
    "])\n",
    "# text_clf.fit(df['words'], df['label'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(df['words'][:400], df['label'][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4425170685088869"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__C: 10\n",
      "tfidf__use_idf: False\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-2, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(df['words'], df['label'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(test['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = test['id']\n",
    "sub_df[\"pred\"] = predicted\n",
    "sub_df.to_csv(\"sgd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'excel stay hotel monaco past delight recept staff friendli profession room smart comfort bed particularli like recept small dog receiv staff guest spoke love mild neg distanc uphil ppmarket restaur 1st overal great experi'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['words'][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 70839\n",
      "Training Set Size: 16000\n",
      "Test Set Size: 4491\n"
     ]
    }
   ],
   "source": [
    "train_id_list, train_data_label, train_data_matrix, vocab = read_data(\"all/train.csv\")\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"Training Set Size:\", len(train_id_list))\n",
    "test_id_list, _, test_data_matrix, _ = read_data(\"all/test.csv\", vocab)\n",
    "print(\"Test Set Size:\", len(test_id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf__alpha': [1e-2, 1e-1, 1],\n",
    "              'clf__loss' : ['hinge', 'perceptron']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "        ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial', class_weight='balanced')),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(train_data_matrix[:400], train_data_label[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4593276337150476"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.1\n",
      "clf__loss: 'hinge'\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_data_matrix, train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result = rf.predict(test_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = pd.read_csv('log_stem.csv')\n",
    "cnn = pd.read_csv('cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034958806501892675"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(stem['pred'] == cnn['pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7742150968603875"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(eclf_result == log['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = test_id_list\n",
    "sub_df[\"pred\"] = eclf_result\n",
    "sub_df.to_csv(\"ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
